# game-review-classification-sbert-xgboost
A project that classifies game reviews using Sentence Transformers (SBERT) and Machine Learning models. The first approach uses all-MiniLM-L6-v2 with Logistic Regression, and the second approach improves accuracy with paraphrase-mpnet-base-v2 and XGBoost.
------
# ğŸ® Game Review Classification using Sentence Transformers & XGBoost

## ğŸ“Œ Overview
This project aims to **classify game reviews** as **Positive (1) or Negative (0)** based on user suggestions using **Sentence Transformers (SBERT) for text embeddings** and **Machine Learning models** for classification.

### **ğŸ”¥ Implemented Two Models:**
1ï¸âƒ£ **SBERT (`all-MiniLM-L6-v2`) + Logistic Regression** â†’ **82% Accuracy**  
2ï¸âƒ£ **SBERT (`paraphrase-mpnet-base-v2`) + XGBoost** â†’ **87% Accuracy**  

âœ… **Uses pre-trained Transformer models for feature extraction**  
âœ… **Works with raw text reviews without heavy preprocessing**  
âœ… **Fast & scalable for large datasets**  

---

## ğŸ“Š Model Performance

| Model | Sentence Transformer Used | Classifier | Accuracy |
|--------|------------------------|------------|----------|
| **Baseline** | `all-MiniLM-L6-v2` | Logistic Regression | **82.22%** |
| **Optimized** | `paraphrase-mpnet-base-v2` | XGBoost | **87.00%** |

---

## ğŸš€ Installation & Setup

### **1ï¸âƒ£ Install Dependencies**
Make sure you have **Python 3.9+** and install the required libraries:

pip install -U sentence-transformers xgboost scikit-learn pandas numpy


### **2ï¸âƒ£ Load the Dataset**
If your dataset is stored in Google Drive, **mount it**:

from google.colab import drive
drive.mount('/content/drive')


âœ” **Dataset Structure:**
- `"user_review"` â†’ Contains the game review text.
- `"user_suggestion"` â†’ The sentiment label (0 = Negative, 1 = Positive).

---

## ğŸ› ï¸ Model Training & Evaluation

### **1ï¸âƒ£ Load Pre-Trained Sentence Transformer**

from sentence_transformers import SentenceTransformer

# Load the optimized SBERT model
model = SentenceTransformer("paraphrase-mpnet-base-v2")


### **2ï¸âƒ£ Encode Game Reviews into Embeddings**

import pandas as pd

# Load dataset
df = pd.read_csv("/content/drive/My Drive/Colab Notebooks/datasets/game_review/train.csv")

# Convert reviews to embeddings
embeddings = model.encode(df["user_review"].tolist())


---

## **ğŸ¯ Model 1: Logistic Regression (`all-MiniLM-L6-v2`)**

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(embeddings, df["user_suggestion"], test_size=0.2, random_state=42)

# Train Logistic Regression
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

# Evaluate
y_pred = clf.predict(X_test)
print(f"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred):.4f}")

âœ” **Achieved Accuracy:** `82.22%`

---

## **ğŸ”¥ Model 2: XGBoost (`paraphrase-mpnet-base-v2`)**

from xgboost import XGBClassifier

# Train XGBoost classifier
xgb_clf = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, tree_method='gpu_hist')  # Enables GPU training
xgb_clf.fit(X_train, y_train)

# Evaluate XGBoost
y_pred_xgb = xgb_clf.predict(X_test)
print(f"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}")

âœ” **Achieved Accuracy:** `87.00%` ğŸ¯

---

## **ğŸ” Finding Similar Reviews Using Cosine Similarity**
Use **cosine similarity** to find the most similar review to a given input.


from sentence_transformers import util
import numpy as np

query = "This game is fantastic!"
query_embedding = model.encode(query, device='cuda')  # Move to GPU

# Compute similarity with existing embeddings
similarities = util.cos_sim(query_embedding, embeddings)

# Get most similar review
most_similar_idx = np.argmax(similarities).item()
print(f"Most similar review: {df.iloc[most_similar_idx]['user_review']}")

âœ” **This retrieves the most similar review from the dataset.**

---

## ğŸ“Œ Key Features & Learnings
âœ” **Sentence Transformers simplify text classification** by generating **dense vector embeddings**.  
âœ” **SBERT (`paraphrase-mpnet-base-v2`) captures better contextual meaning** than `all-MiniLM-L6-v2`.  
âœ” **XGBoost outperforms Logistic Regression in accuracy**.  
âœ” **Finding similar reviews using cosine similarity can help in duplicate detection.**

---

## ğŸ“‚ Folder Structure
```
ğŸ“ game-review-classification-sentence-transformer/
 â”œâ”€â”€ ğŸ“„ game_reviews.csv           # Dataset file
 â”œâ”€â”€ ğŸ“œ train_logistic_regression.py  # Baseline model (SBERT + Logistic Regression)
 â”œâ”€â”€ ğŸ“œ train_xgboost.py          # Optimized model (SBERT + XGBoost)
 â”œâ”€â”€ ğŸ“œ find_similar_reviews.py   # Code for similarity search
 â”œâ”€â”€ ğŸ“„ README.md                 # Project Documentation
```

---

## ğŸ’¡ Future Improvements
ğŸ”¹ Fine-tune `paraphrase-mpnet-base-v2` on gaming-specific reviews.  
ğŸ”¹ Experiment with **LightGBM or CatBoost** for classification.  
ğŸ”¹ Use **FAISS** for faster nearest neighbor search in large datasets.  
ğŸ”¹ Apply **data augmentation** to handle class imbalance.  

---

## ğŸ“œ References
- [Sentence Transformers](https://www.sbert.net)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Scikit-Learn](https://scikit-learn.org/)

---

## ğŸ‘¨â€ğŸ’» Author & Contribution
Created by **Govinda Tak** ğŸ®ğŸš€  
Feel free to contribute, raise issues, or suggest improvements!  

ğŸ“Œ **GitHub Repository Link:** ğŸ”—   (https://github.com/GovindaTak/game-review-classification-sbert-xgboost.git)
---

### **ğŸ“Œ Summary**
âœ” **Two approaches: Logistic Regression & XGBoost for classification.**  
âœ” **Uses SBERT (`paraphrase-mpnet-base-v2`) for better embeddings.**  
âœ” **Implemented similarity detection using cosine similarity.**  
âœ” **Repository structured with scripts for easy execution.**  
